{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVyPXo75QJiZ",
        "outputId": "d779e256-0506-438b-d982-774918958a3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-1.3.4-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.64.0)\n",
            "Collecting underthesea-core==0.0.4_alpha.10\n",
            "  Downloading underthesea_core-0.0.4_alpha.10-cp37-cp37m-manylinux2010_x86_64.whl (581 kB)\n",
            "\u001b[K     |████████████████████████████████| 581 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Installing collected packages: unidecode, underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.3.4 underthesea-core-0.0.4a10 unidecode-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from underthesea import sent_tokenize"
      ],
      "metadata": {
        "id": "Q-7KqlB23zXZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Thiên nhiên Tây Bắc được tô điểm bởi con sông Đà vừa hung bạo vừa trữ tình.\\\n",
        "        Sông Đà có lúc dịu dàng như người phụ nữ kiều diễm. \\\n",
        "        Nước sông Đà thay đổi theo mùa, phản chiếu trời xuân nắng thu: “Mùa xuân dòng xanh ngọc bích, mùa thu lừ lừ chín đỏ như da mặt người bầm đi vì rượu bữa”. \\\n",
        "        Dọc theo sông Đà, có lắm thác nhiều ghềnh, có đá dựng vách thành, có đá tảng, đá hòn bày thế thạch trận, tạo nên cửa sinh, cửa tử. \\\n",
        "        Nổi bật trên bức tranh thiên nhiên hùng vĩ, đầy sức sống đó là hình ảnh ông lái đò sông Đà. \\\n",
        "        Đó là một người mang vẻ đẹp khỏe khoắn của người dân lao động vùng sông nước với thân hình cao to, nước da rám nắng. \\\n",
        "        Ông làm nghề lái đò đã nhiều năm, từng gắn bó với dòng sông Đà, hiểu được tính khí của nó. \\\n",
        "        Ông thuộc nằm lòng từng con thác lớn, thác nhỏ, từng vách đá, luồng nước, từng cửa sinh, cửa tử do thế thạch trận tạo nên. \\\n",
        "        Ông đã dùng kinh nghiệm nghề nghiệp cộng với sự cần cù gan dạ đưa con thuyền vượt thác nước sông Đà đầy nguy hiểm. \\\n",
        "        Ông đã đưa nhiều chuyến hàng về xuôi an toàn để góp phần vào cuộc sống. \\\n",
        "        Sau khi vượt sông Đà, ông lái đò trở về cuộc sống đời thường thanh thản của mình, ông neo thuyền chỗ khúc sông bình lặng, nấu ống cơm lam và bàn tán về cá anh vũ, cá dầm xanh.\"\n",
        "\n",
        "corpus = []\n",
        "for sentence in sent_tokenize(text):\n",
        "  sent = []\n",
        "  for word in sentence.split(' '):\n",
        "    tmp = ''.join(c for c in word if c.isalpha())\n",
        "    sent.append(tmp.lower())\n",
        "  corpus.append(sent)\n",
        "\n",
        "for i in corpus:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JLyLxdRydXR",
        "outputId": "896f34f3-a465-4a5e-a22a-d1631f4cd834"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thiên', 'nhiên', 'tây', 'bắc', 'được', 'tô', 'điểm', 'bởi', 'con', 'sông', 'đà', 'vừa', 'hung', 'bạo', 'vưa', 'trữ', 'tình']\n",
            "['sông', 'đà', 'có', 'lúc', 'dịu', 'dàng', 'như', 'người', 'phụ', 'nữ', 'kiều', 'diễm']\n",
            "['nước', 'sông', 'đà', 'thay', 'đổi', 'theo', 'mùa', 'phản', 'chiếu', 'trời', 'xuân', 'nắng', 'thu', 'mùa', 'xuân', 'dòng', 'xanh', 'ngọc', 'bích', 'mùa', 'thu', 'lừ', 'lừ', 'chín', 'đỏ', 'như', 'da', 'mặt', 'người', 'bầm', 'đi', 'vì', 'rượu', 'bữa']\n",
            "['dọc', 'theo', 'sông', 'đà', 'có', 'lắm', 'thác', 'nhiều', 'ghềnh', 'có', 'đá', 'dựng', 'vách', 'thành', 'có', 'đá', 'tảng', 'đá', 'hòn', 'bày', 'thế', 'thạch', 'trận', 'tạo', 'nên', 'cửa', 'sinh', 'cửa', 'tử']\n",
            "['nổi', 'bật', 'trên', 'bức', 'tranh', 'thiên', 'nhiên', 'hùng', 'vĩ', 'đầy', 'sức', 'sống', 'đó', 'là', 'hình', 'ảnh', 'ông', 'lái', 'đò', 'sông', 'đà']\n",
            "['đó', 'là', 'một', 'người', 'mang', 'vẻ', 'đẹp', 'khỏe', 'khoắn', 'của', 'người', 'dân', 'lao', 'động', 'vùng', 'sông', 'nước', 'với', 'thân', 'hình', 'cao', 'to', 'nước', 'da', 'rám', 'nắng']\n",
            "['ông', 'làm', 'nghề', 'lái', 'đò', 'đã', 'nhiều', 'năm', 'từng', 'gắn', 'bó', 'với', 'dòng', 'sông', 'đà', 'hiểu', 'được', 'tính', 'khí', 'của', 'nó']\n",
            "['ông', 'thuộc', 'nằm', 'lòng', 'từng', 'con', 'thác', 'lớn', 'thác', 'nhỏ', 'từng', 'vách', 'đá', 'luồng', 'nước', 'từng', 'cửa', 'sinh', 'cửa', 'tử', 'do', 'thế', 'thạch', 'trận', 'tạo', 'nên']\n",
            "['ông', 'đã', 'dùng', 'kinh', 'nghiệm', 'nghề', 'nghiệp', 'cộng', 'với', 'sự', 'cần', 'cù', 'gan', 'dạ', 'đưa', 'con', 'thuyền', 'vượt', 'thác', 'nước', 'sông', 'đà', 'đầy', 'nguy', 'hiểm']\n",
            "['ông', 'đã', 'đưa', 'nhiều', 'chuyến', 'hàng', 'về', 'xuôi', 'an', 'toàn', 'để', 'góp', 'phần', 'vào', 'cuộc', 'sống']\n",
            "['sau', 'khi', 'vượt', 'sông', 'đà', 'ông', 'lái', 'đò', 'trở', 'về', 'cuộc', 'sống', 'đời', 'thường', 'thanh', 'thản', 'của', 'mình', 'ông', 'neo', 'thuyền', 'chỗ', 'khúc', 'sông', 'bình', 'lặng', 'nấu', 'ống', 'cơm', 'lam', 'và', 'bàn', 'tán', 'về', 'cá', 'anh', 'vũ', 'cá', 'dầm', 'xanh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window = 2\n",
        "\n",
        "vocabulary = defaultdict(int)\n",
        "\n",
        "for row in corpus:\n",
        "  for word in row:\n",
        "    vocabulary[word] += 1\n",
        "\n",
        "vocab_count = len(vocabulary.keys())\n",
        "words_list = list(vocabulary.keys())\n",
        "\n",
        "word_index = dict((word, i) for i, word in enumerate(words_list))\n",
        "index_word = dict((i, word) for i, word in enumerate(words_list))\n",
        "\n",
        "\n",
        "def one_hot_vector(word):\n",
        "  word_vector = np.zeros(vocab_count, dtype = int)\n",
        "  w_index = word_index[word]\n",
        "  word_vector[w_index] = 1\n",
        "  return word_vector\n",
        "\n",
        "\n",
        "training_data = []\n",
        "\n",
        "for sentence in corpus:\n",
        "  for i, word in enumerate(sentence):\n",
        "    target_word = list(one_hot_vector(sentence[i]))\n",
        "    context_word = []\n",
        "    for j in range(i - window, i + window+1):\n",
        "      if j != i and j <= len(sentence)-1 and j >= 0:\n",
        "        context_word.append(list(one_hot_vector(sentence[j])))\n",
        "\n",
        "    training_data.append([target_word, context_word])\n",
        "\n",
        "training_data = np.array(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3qyS6s-nhq",
        "outputId": "86e775d1-8d95-4384-e805-5c2fa5266706"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3JM1tFbD9eg",
        "outputId": "f95fc8e2-d0a6-461e-b555-eae8f50e6393"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  list([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
            " [list([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  list([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
            " [list([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  list([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
            " [list([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  list([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
            " [list([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  list([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = np.random.uniform(-1, 1, (vocab_count, 340))\n",
        "w2 = np.random.uniform(-1, 1, (340, vocab_count))\n",
        "print(w1)\n",
        "print(w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB8SmFg3Ozr_",
        "outputId": "e3e0069b-c40d-423e-88c6-f54ac7232c7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.16531902 -0.87220899 -0.50164461 ...  0.5199511   0.8782541\n",
            "  -0.14311612]\n",
            " [ 0.13864119  0.15530534  0.61075291 ...  0.35300362 -0.60585713\n",
            "  -0.6461554 ]\n",
            " [ 0.19794942  0.26136208 -0.99246006 ... -0.52072958  0.23420092\n",
            "  -0.17802126]\n",
            " ...\n",
            " [ 0.57012085  0.28842122  0.11873865 ...  0.20335011 -0.69833778\n",
            "  -0.39721516]\n",
            " [ 0.12433948 -0.55050781  0.21613329 ...  0.01384812 -0.84799913\n",
            "   0.39466537]\n",
            " [ 0.14542455 -0.05170698  0.88669471 ... -0.10519414 -0.13051693\n",
            "   0.81946459]]\n",
            "[[-0.71062379 -0.6794331  -0.36133648 ...  0.66047949  0.10692396\n",
            "   0.90101119]\n",
            " [-0.33594424  0.94904051  0.65318802 ... -0.99024048 -0.63584621\n",
            "  -0.34390046]\n",
            " [-0.14550484  0.14081863  0.05915987 ... -0.03550059 -0.09701657\n",
            "  -0.67997864]\n",
            " ...\n",
            " [-0.67898715 -0.6076477   0.81268594 ... -0.31717984  0.83256916\n",
            "   0.46422125]\n",
            " [-0.27200243 -0.20686281 -0.94233797 ...  0.64083894  0.17755164\n",
            "  -0.8064829 ]\n",
            " [ 0.6531925   0.6421361   0.9998599  ... -0.28967786  0.86349743\n",
            "  -0.35890299]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Skip-gram**"
      ],
      "metadata": {
        "id": "tX5tS3ThpEO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  sum = 0\n",
        "  for i in x:\n",
        "    sum += np.exp(i)\n",
        "  result = []\n",
        "  for i in x:\n",
        "    result.append(np.exp(i)/sum)\n",
        "  return np.array(result)"
      ],
      "metadata": {
        "id": "6lMw07UI2rRm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 50\n",
        "\n",
        "for i in range(epoch):\n",
        "  loss = 0\n",
        "  for target_word, context_word in training_data:\n",
        "    matrix1 = np.dot(w1.T, target_word)\n",
        "    matrix2 = np.dot(w2.T, matrix1)\n",
        "    y_pred = softmax(matrix2)\n",
        "\n",
        "    error = np.sum([np.subtract(y_pred, word) for word in context_word], axis=0)\n",
        "\n",
        "    w2_new = np.outer(matrix1, error)\n",
        "    w1_new = np.outer(target_word, np.dot(w2, error.T))\n",
        "    # Cập nhật weights với learning rate = 0.05\n",
        "    w1 = w1 - (0.05 * w1_new)\n",
        "    w2 = w2 - (0.05 * w2_new)\n",
        "\n",
        "    loss += -np.sum([matrix2[word.index(1)] for word in context_word]) + len(context_word) * np.log(np.sum(np.exp(matrix2)))\n",
        "  print('Epoch:', i, \"Loss:\", loss)"
      ],
      "metadata": {
        "id": "O2SUIHbIQuVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4976a916-d800-456a-b242-140e84c9f5d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 16765.098286221983\n",
            "Epoch: 1 Loss: 11449.464586366967\n",
            "Epoch: 2 Loss: 12975.524940315576\n",
            "Epoch: 3 Loss: 14816.837270766786\n",
            "Epoch: 4 Loss: 14522.778512442914\n",
            "Epoch: 5 Loss: 12507.502354833887\n",
            "Epoch: 6 Loss: 11550.373892949889\n",
            "Epoch: 7 Loss: 11529.664108833584\n",
            "Epoch: 8 Loss: 10991.272690433896\n",
            "Epoch: 9 Loss: 9826.146830715983\n",
            "Epoch: 10 Loss: 9443.884068200921\n",
            "Epoch: 11 Loss: 9285.3364425283\n",
            "Epoch: 12 Loss: 9056.289700580779\n",
            "Epoch: 13 Loss: 8304.420643189109\n",
            "Epoch: 14 Loss: 7702.575625272299\n",
            "Epoch: 15 Loss: 7771.361174136527\n",
            "Epoch: 16 Loss: 7341.400868072191\n",
            "Epoch: 17 Loss: 6813.1483132500825\n",
            "Epoch: 18 Loss: 6721.985919763669\n",
            "Epoch: 19 Loss: 6614.122534655232\n",
            "Epoch: 20 Loss: 6281.721552693033\n",
            "Epoch: 21 Loss: 5973.879880583758\n",
            "Epoch: 22 Loss: 5877.603657431207\n",
            "Epoch: 23 Loss: 5717.866067915828\n",
            "Epoch: 24 Loss: 5523.460024967724\n",
            "Epoch: 25 Loss: 5292.466752712139\n",
            "Epoch: 26 Loss: 5207.110259349875\n",
            "Epoch: 27 Loss: 5125.469061089498\n",
            "Epoch: 28 Loss: 4958.086866170333\n",
            "Epoch: 29 Loss: 4796.78021501479\n",
            "Epoch: 30 Loss: 4679.61196053717\n",
            "Epoch: 31 Loss: 4646.94844675013\n",
            "Epoch: 32 Loss: 4498.984835716682\n",
            "Epoch: 33 Loss: 4338.276974108958\n",
            "Epoch: 34 Loss: 4284.319829450922\n",
            "Epoch: 35 Loss: 4174.168821745665\n",
            "Epoch: 36 Loss: 4025.7498453254584\n",
            "Epoch: 37 Loss: 3925.2712520933856\n",
            "Epoch: 38 Loss: 3835.2451268813766\n",
            "Epoch: 39 Loss: 3768.446272146428\n",
            "Epoch: 40 Loss: 3624.872624799088\n",
            "Epoch: 41 Loss: 3519.267872170185\n",
            "Epoch: 42 Loss: 3430.1052925249005\n",
            "Epoch: 43 Loss: 3293.52501997806\n",
            "Epoch: 44 Loss: 3144.2136357876284\n",
            "Epoch: 45 Loss: 3083.0549717299027\n",
            "Epoch: 46 Loss: 3018.7717959041947\n",
            "Epoch: 47 Loss: 2931.3093071777416\n",
            "Epoch: 48 Loss: 2851.113042658782\n",
            "Epoch: 49 Loss: 2794.16762754086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tìm n context word với đầu vào là target word\n",
        "from collections import Counter\n",
        "\n",
        "def find_context_word(word, n):\n",
        "  w_index = word_index[word]\n",
        "  v_w1 = w1[w_index]\n",
        "  word_sim = Counter()\n",
        "\n",
        "  for i in range(vocab_count):\n",
        "    # Find the similary score for each word in vocab\n",
        "    v_w2 = w1[i]\n",
        "    #Đo độ tương đồng bằng tích vô hướng\n",
        "    similar = np.dot(v_w1.T, v_w2)\n",
        "\n",
        "    word = index_word[i]\n",
        "    word_sim[word] = similar\n",
        "\n",
        " # words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "  return word_sim.most_common(n)\n",
        "\n",
        "find_context_word('sông', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8lk_LbQSCm3",
        "outputId": "b8449e27-a81e-41ee-c846-2b6970f1a400"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sông', 264.79792570896586),\n",
              " ('đà', 118.31827350089767),\n",
              " ('nước', 95.48747581398894),\n",
              " ('có', 55.07278673684312),\n",
              " ('ông', 49.939432523524886)]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CBOW**"
      ],
      "metadata": {
        "id": "xUf35PVA2uB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 50\n",
        "\n",
        "for i in range(epoch):\n",
        "  loss = 0\n",
        "  for target_word, context_word in training_data:\n",
        "    sum = 0\n",
        "    for word in context_word:\n",
        "      sum += np.array(word)\n",
        "\n",
        "    m1 = sum/len(context_word)\n",
        "\n",
        "    matrix1 = np.dot(w1.T, m1)\n",
        "    matrix2 = np.dot(w2.T, matrix1)\n",
        "\n",
        "    y_pred = softmax(matrix2)\n",
        "\n",
        "    error = np.subtract(y_pred, target_word)\n",
        "\n",
        "    w2_new = np.outer(matrix1, error)\n",
        "    w1_new = np.outer(m1, np.dot(w2, error.T))\n",
        "    # Cập nhật weights với learning rate = 0.05\n",
        "    w1 = w1 - (0.05 * w1_new)\n",
        "    w2 = w2 - (0.05 * w2_new)\n",
        "\n",
        "    loss += -matrix2[target_word.index(1)] + np.log(np.sum(np.exp(matrix2)))\n",
        "  print('Epoch:', i, \"Loss:\", loss)"
      ],
      "metadata": {
        "id": "pP2qKX5HGC2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86fcdf2-b9bc-42a5-e249-bf597cbd8297"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 531.4513796055671\n",
            "Epoch: 1 Loss: 405.9455057571848\n",
            "Epoch: 2 Loss: 309.6657099243893\n",
            "Epoch: 3 Loss: 236.44620122507095\n",
            "Epoch: 4 Loss: 182.1896086398146\n",
            "Epoch: 5 Loss: 142.62522440182602\n",
            "Epoch: 6 Loss: 113.88568973240432\n",
            "Epoch: 7 Loss: 92.83324156499319\n",
            "Epoch: 8 Loss: 77.22160457119611\n",
            "Epoch: 9 Loss: 65.5066987826477\n",
            "Epoch: 10 Loss: 56.59120039011937\n",
            "Epoch: 11 Loss: 49.67066473879267\n",
            "Epoch: 12 Loss: 44.18926827691392\n",
            "Epoch: 13 Loss: 39.766965170455144\n",
            "Epoch: 14 Loss: 36.13510599260249\n",
            "Epoch: 15 Loss: 33.10122465715036\n",
            "Epoch: 16 Loss: 30.527988668914983\n",
            "Epoch: 17 Loss: 28.31711060999075\n",
            "Epoch: 18 Loss: 26.397161480612148\n",
            "Epoch: 19 Loss: 24.71512548306998\n",
            "Epoch: 20 Loss: 23.230730341184525\n",
            "Epoch: 21 Loss: 21.91261714149583\n",
            "Epoch: 22 Loss: 20.735751184524617\n",
            "Epoch: 23 Loss: 19.67970512068889\n",
            "Epoch: 24 Loss: 18.727545791157066\n",
            "Epoch: 25 Loss: 17.865107402537213\n",
            "Epoch: 26 Loss: 17.08048794599607\n",
            "Epoch: 27 Loss: 16.363666821431934\n",
            "Epoch: 28 Loss: 15.706192339809554\n",
            "Epoch: 29 Loss: 15.100918176984711\n",
            "Epoch: 30 Loss: 14.541780858898022\n",
            "Epoch: 31 Loss: 14.023613867269338\n",
            "Epoch: 32 Loss: 13.541994090017182\n",
            "Epoch: 33 Loss: 13.09311600563147\n",
            "Epoch: 34 Loss: 12.673689020034624\n",
            "Epoch: 35 Loss: 12.280853770669173\n",
            "Epoch: 36 Loss: 11.912113792587371\n",
            "Epoch: 37 Loss: 11.565279555060629\n",
            "Epoch: 38 Loss: 11.238422442094649\n",
            "Epoch: 39 Loss: 10.929836733557199\n",
            "Epoch: 40 Loss: 10.638008040584635\n",
            "Epoch: 41 Loss: 10.361586967449725\n",
            "Epoch: 42 Loss: 10.099367024393974\n",
            "Epoch: 43 Loss: 9.850266014544038\n",
            "Epoch: 44 Loss: 9.613310274034845\n",
            "Epoch: 45 Loss: 9.387621267050637\n",
            "Epoch: 46 Loss: 9.172404134038786\n",
            "Epoch: 47 Loss: 8.96693786763776\n",
            "Epoch: 48 Loss: 8.770566851376543\n",
            "Epoch: 49 Loss: 8.582693544426025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_target_word(words):\n",
        "\n",
        "  result = Counter()\n",
        "\n",
        "  for w in words:\n",
        "    re = Counter()\n",
        "    for i in find_context_word(w, vocab_count):\n",
        "      re[i[0]] = i[1]\n",
        "    result += re\n",
        "  \n",
        "  return result.most_common(3)\n",
        "  \n",
        "\n",
        "list_context_word = ['dọc', 'sông']\n",
        "print(find_target_word(list_context_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s2wrjXwvhp8",
        "outputId": "114e5805-6caa-4c79-aa53-8e33bf00eb8e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('sông', 285.09241521658555), ('dọc', 190.64244523767573), ('đà', 118.31827350089767)]\n"
          ]
        }
      ]
    }
  ]
}